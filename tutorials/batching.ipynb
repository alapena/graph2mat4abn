{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2fb480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/miniconda3/envs/graph2mat/lib/python3.12/site-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import importlib\n",
    "\n",
    "# So that we can plot sisl geometries\n",
    "import sisl.viz\n",
    "\n",
    "from e3nn import o3\n",
    "from pathlib import Path\n",
    "\n",
    "from graph2mat import (\n",
    "    PointBasis,\n",
    "    BasisTableWithEdges,\n",
    "    BasisConfiguration,\n",
    "    MatrixDataProcessor,\n",
    ")\n",
    "from graph2mat.bindings.torch import TorchBasisMatrixData\n",
    "from graph2mat.bindings.e3nn import E3nnGraph2Mat\n",
    "\n",
    "def load_config(path=\"../config.yaml\"):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813484d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building the basis. Breaking...\n",
      "Basis built!\n",
      "\n",
      "Basis for atom 0.\n",
      "\tAtom type: 5\n",
      "\tBasis: ((2, 0, 1), (2, 1, -1), (1, 2, 1))\n",
      "\tBasis convention: siesta_spherical\n",
      "\tR: [3.02420918 2.02341372 3.73961942 3.73961942 3.73961942 2.51253945\n",
      " 2.51253945 2.51253945 3.73961942 3.73961942 3.73961942 3.73961942\n",
      " 3.73961942]\n",
      "\n",
      "Basis for atom 1.\n",
      "\tAtom type: 6\n",
      "\tBasis: ((2, 0, 1), (2, 1, -1), (1, 2, 1))\n",
      "\tBasis convention: siesta_spherical\n",
      "\tR: [2.57112067 1.67068795 3.16682115 3.16682115 3.16682115 2.05819653\n",
      " 2.05819653 2.05819653 3.16682115 3.16682115 3.16682115 3.16682115\n",
      " 3.16682115]\n",
      "\n",
      "Basis for atom 2.\n",
      "\tAtom type: 7\n",
      "\tBasis: ((2, 0, 1), (2, 1, -1), (1, 2, 1))\n",
      "\tBasis convention: siesta_spherical\n",
      "\tR: [2.25704422 1.4271749  2.78012609 2.78012609 2.78012609 1.75309697\n",
      " 1.75309697 1.75309697 2.78012609 2.78012609 2.78012609 2.78012609\n",
      " 2.78012609]\n"
     ]
    }
   ],
   "source": [
    "# === List of paths to all structures ===\n",
    "parent_path = Path('../dataset')\n",
    "n_atoms_paths = list(parent_path.glob('*/'))\n",
    "paths = []\n",
    "for n_atoms_path in n_atoms_paths:\n",
    "    structure_paths = list(n_atoms_path.glob('*/'))\n",
    "    paths.append(structure_paths)\n",
    "paths = flatten(paths)\n",
    "\n",
    "\n",
    "# === Config init ===\n",
    "config = load_config()\n",
    "model_config = config[\"model\"]\n",
    "orbitals = config['orbitals']\n",
    "device = torch.device(config[\"device\"] if (torch.cuda.is_available() and config[\"device\"]!=\"cpu\") \n",
    "else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "# == Basis === \n",
    "basis = []\n",
    "unique_atom_types = []\n",
    "\n",
    "# We need a basis that contains all atom types in our dataset. To do so, we will examine just one sample of 64 atoms, because there already are atoms of all types.\n",
    "for i, path in enumerate(paths):\n",
    "    if 'SHARE_OUTPUTS_64_ATOMS' in str(path):\n",
    "        file = sisl.get_sile(path / \"aiida.fdf\")\n",
    "        geometry = file.read_geometry()\n",
    "        for atom in geometry.atoms:\n",
    "            # Boron, Carbon, Nitrogen\n",
    "            if (atom.Z == 5 or atom.Z == 6 or atom.Z == 7) and atom.Z not in unique_atom_types:\n",
    "                basis.append(PointBasis.from_sisl_atom(atom))\n",
    "                unique_atom_types.append(atom.Z)\n",
    "            if len(unique_atom_types) == 3:\n",
    "                break\n",
    "        if len(unique_atom_types) == 3:\n",
    "                print(\"Finished building the basis. Breaking...\")\n",
    "                break\n",
    "\n",
    "basis.sort(key=lambda x: x.type)\n",
    "unique_atom_types.sort()\n",
    "\n",
    "# Check that there are 3 atoms in the basis.\n",
    "if len(basis) != 3:\n",
    "    raise ValueError(\"There are not three elements in the basis\")\n",
    "\n",
    "print(\"Basis built!\")\n",
    "[print(f\"\\nBasis for atom {i}.\\n\\tAtom type: {basis[i].type}\\n\\tBasis: {basis[i].basis}\\n\\tBasis convention: {basis[i].basis_convention}\\n\\tR: {basis[i].R}\") for i in range(len(basis))]\n",
    "\n",
    "# === Basis table === \n",
    "table = BasisTableWithEdges(basis)\n",
    "\n",
    "# === Data processor ===\n",
    "# Initialize the processor.\n",
    "processor = MatrixDataProcessor(\n",
    "    basis_table=table, symmetric_matrix=True, sub_point_matrix=False, out_matrix=None\n",
    ")\n",
    "\n",
    "# === Shape of our inputs ===\n",
    "hidden_irreps = o3.Irreps(model_config[\"atomic_descriptors\"][\"hidden_irreps\"])\n",
    "num_interactions = model_config[\"atomic_descriptors\"][\"num_interactions\"]\n",
    "final_irreps = o3.Irreps([(mul, ir) for mul, ir in hidden_irreps] * num_interactions)\n",
    "\n",
    "# === Enviroment representation ===\n",
    "from mace.modules import RadialEmbeddingBlock, EquivariantProductBasisBlock\n",
    "from mace.modules.utils import get_edge_vectors_and_lengths\n",
    "\n",
    "class EmbeddingBase(torch.nn.Module):\n",
    "    def __init__(self, config, orbitals):\n",
    "        super(EmbeddingBase, self).__init__()\n",
    "\n",
    "        embeddings_config = config[\"model\"][\"embedding\"]\n",
    "        self.device = config[\"device\"]\n",
    "\n",
    "        # Define the irreducible representations for the node attributes and features.\n",
    "        node_attr_irreps = o3.Irreps([(embeddings_config[\"num_elements\"], (0, 1))]) # E.g. [(10, (0,1))]\n",
    "        hidden_irreps = o3.Irreps(embeddings_config[\"hidden_irreps\"]) # E.g. \"8x0e+8x1o\"\n",
    "        node_feats_irreps = o3.Irreps([(hidden_irreps.count(o3.Irrep(0, 1)), (0, 1))]) # Counting how many Irrep(0, 1) there are inside hidden_irreps.\n",
    "\n",
    "        # Linear transformation from node attributes to node features.\n",
    "        # / I think this is the same as torch.nn.Linear\n",
    "        self.node_embedding = o3.Linear(\n",
    "            node_attr_irreps,\n",
    "            node_feats_irreps,\n",
    "            shared_weights=True,\n",
    "            internal_weights=True,\n",
    "        )\n",
    "\n",
    "        # Radial embedding block using Bessel functions and polynomial cutoffs.\n",
    "        self.radial_embedding = RadialEmbeddingBlock(\n",
    "            r_max=embeddings_config[\"r_max\"],\n",
    "            num_bessel=embeddings_config[\"num_bessel\"],\n",
    "            num_polynomial_cutoff=embeddings_config[\"num_polynomial_cutoff\"],\n",
    "            radial_type=embeddings_config[\"radial_type\"],\n",
    "            distance_transform=embeddings_config[\"distance_transform\"],\n",
    "        )\n",
    "\n",
    "        # Angular embedding using spherical harmonics.\n",
    "        sh_irreps = o3.Irreps.spherical_harmonics(embeddings_config[\"max_ell\"])\n",
    "        self.angular_embedding = o3.SphericalHarmonics(sh_irreps, normalize=True, normalization=\"component\")\n",
    "\n",
    "        # Element encoding configuration\n",
    "        self.orbitals = orbitals\n",
    "        self.nr_bit = embeddings_config[\"nr_bits\"]\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        # Encode atomic numbers into binary orbital-based representation. \n",
    "        atom_types = data.metadata['atom_types'].unsqueeze(1) # The reshape is for format reasons\n",
    "        one_hot_z = z_one_hot(atom_types, orbitals=self.orbitals, nr_bits=self.nr_bit).to(self.device)\n",
    "\n",
    "        # Input node descriptors.\n",
    "        node_feats = one_hot_z\n",
    "\n",
    "        # Calculate edge vectors and their lengths (distances).\n",
    "        vectors, lengths = get_edge_vectors_and_lengths(\n",
    "            positions=data.positions,\n",
    "            edge_index=data.edge_index,\n",
    "            shifts=data.shifts,\n",
    "        )\n",
    "\n",
    "        # Apply node embedding.\n",
    "        node_feats = self.node_embedding(node_feats)\n",
    "        \n",
    "\n",
    "        # Apply radial and angular embeddings for edges.\n",
    "        radial_embedding = self.radial_embedding(\n",
    "            lengths,\n",
    "            node_feats,\n",
    "            data.edge_index,\n",
    "            atom_types\n",
    "        )\n",
    "        angular_embedding = self.angular_embedding(vectors)\n",
    "\n",
    "        # Bundle the embeddings.\n",
    "        embedding_collection = {\n",
    "            \"nodes\": {\n",
    "                \"one_hot\": one_hot_z,\n",
    "                \"node_features\": node_feats,\n",
    "            },\n",
    "            \"edges\": {\n",
    "                \"radial_embedding\": radial_embedding,\n",
    "                \"angular_embedding\": angular_embedding,\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return embedding_collection\n",
    "    \n",
    "\n",
    "class MACEDescriptor(torch.nn.Module):\n",
    "    def __init__(self, atomic_descriptors_config):\n",
    "        super(MACEDescriptor, self).__init__()\n",
    "\n",
    "        # --- Irreps definitions ---\n",
    "        node_attr_irreps = o3.Irreps([(atomic_descriptors_config[\"num_elements\"], (0, 1))])  # One-hot per element (scalar-even)\n",
    "\n",
    "        # Extract number of scalar-even irreps from hidden_irreps\n",
    "        hidden_irreps = o3.Irreps(atomic_descriptors_config[\"hidden_irreps\"])\n",
    "        num_scalar_irreps = hidden_irreps.count(o3.Irrep(0, 1))\n",
    "        node_feats_irreps = o3.Irreps([(num_scalar_irreps, (0, 1))])\n",
    "\n",
    "        sh_irreps = o3.Irreps.spherical_harmonics(atomic_descriptors_config[\"max_ell\"])  # Angular features\n",
    "\n",
    "        radial_out_dim = atomic_descriptors_config[\"radial_embedding.out_dim\"]\n",
    "        edge_feats_irreps = o3.Irreps(f\"{radial_out_dim}x0e\")  # Radial embeddings as scalar-even\n",
    "\n",
    "        hidden_irreps_out = hidden_irreps  # Output IRs remain same\n",
    "\n",
    "        # Determine output irreps of interaction (spherical harmonics ⊗ scalar features)\n",
    "        interaction_irreps = (sh_irreps * num_scalar_irreps).sort()[0].simplify()\n",
    "\n",
    "        # Support for correlation order per layer\n",
    "        if isinstance(atomic_descriptors_config[\"correlation\"], int):\n",
    "            correlation = [atomic_descriptors_config[\"correlation\"]] * atomic_descriptors_config[\"num_interactions\"]\n",
    "\n",
    "        # --- First Interaction Layer ---\n",
    "        interaction_cls_first = get_object_from_module(atomic_descriptors_config[\"interaction_cls_first\"], \"mace.modules\")\n",
    "        first_interaction = interaction_cls_first(\n",
    "            node_attrs_irreps=node_attr_irreps,\n",
    "            node_feats_irreps=node_feats_irreps,\n",
    "            edge_attrs_irreps=sh_irreps,\n",
    "            edge_feats_irreps=edge_feats_irreps,\n",
    "            target_irreps=interaction_irreps,\n",
    "            hidden_irreps=hidden_irreps,\n",
    "            avg_num_neighbors=atomic_descriptors_config[\"avg_num_neighbors\"],\n",
    "            radial_MLP=atomic_descriptors_config[\"radial_mlp\"],\n",
    "            cueq_config=None,\n",
    "        )\n",
    "\n",
    "        self.interactions = torch.nn.ModuleList([first_interaction])\n",
    "\n",
    "        # Determine whether to use self-connection (important for residual-based models)\n",
    "        use_sc_first = \"Residual\" in str(atomic_descriptors_config[\"interaction_cls_first\"])\n",
    "\n",
    "        first_product = EquivariantProductBasisBlock(\n",
    "            node_feats_irreps=first_interaction.target_irreps,\n",
    "            target_irreps=hidden_irreps,\n",
    "            correlation=correlation[0],\n",
    "            num_elements=atomic_descriptors_config[\"num_elements\"],\n",
    "            use_sc=use_sc_first,\n",
    "            cueq_config=None,\n",
    "        )\n",
    "\n",
    "        self.products = torch.nn.ModuleList([first_product])\n",
    "\n",
    "        # --- Remaining Interaction-Product Blocks ---\n",
    "        for i in range(atomic_descriptors_config[\"num_interactions\"] - 1):\n",
    "            interaction_cls = get_object_from_module(atomic_descriptors_config[\"interaction_cls\"], \"mace.modules\")\n",
    "            interaction = interaction_cls(\n",
    "                node_attrs_irreps=node_attr_irreps,\n",
    "                node_feats_irreps=hidden_irreps_out,\n",
    "                edge_attrs_irreps=sh_irreps,\n",
    "                edge_feats_irreps=edge_feats_irreps,\n",
    "                target_irreps=interaction_irreps,\n",
    "                hidden_irreps=hidden_irreps_out,\n",
    "                avg_num_neighbors=atomic_descriptors_config[\"avg_num_neighbors\"],\n",
    "                radial_MLP=atomic_descriptors_config[\"radial_mlp\"],\n",
    "                cueq_config=None,\n",
    "            )\n",
    "\n",
    "            product = EquivariantProductBasisBlock(\n",
    "                node_feats_irreps=interaction_irreps,\n",
    "                target_irreps=hidden_irreps_out,\n",
    "                correlation=correlation[i + 1],\n",
    "                num_elements=atomic_descriptors_config[\"num_elements\"],\n",
    "                use_sc=True,\n",
    "                cueq_config=None,\n",
    "            )\n",
    "\n",
    "            self.interactions.append(interaction)\n",
    "            self.products.append(product)\n",
    "\n",
    "    def forward(self, embeddings, edge_index):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            data (_type_): Already preprocessed data.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "\n",
    "        node_feats = embeddings[\"nodes\"][\"node_features\"]\n",
    "\n",
    "        node_feats_list = []\n",
    "        for interaction, product in zip(self.interactions, self.products):\n",
    "            node_feats, sc = interaction(\n",
    "                node_attrs=embeddings[\"nodes\"][\"one_hot\"],\n",
    "                node_feats=node_feats,\n",
    "                edge_attrs=embeddings[\"edges\"][\"angular_embedding\"],\n",
    "                edge_feats=embeddings[\"edges\"][\"radial_embedding\"],\n",
    "                edge_index=edge_index,\n",
    "            )\n",
    "\n",
    "            node_feats = product(\n",
    "                node_feats=node_feats,\n",
    "                sc=sc,\n",
    "                node_attrs=embeddings[\"nodes\"][\"one_hot\"],\n",
    "            )\n",
    "\n",
    "            node_feats_list.append(node_feats)\n",
    "\n",
    "        # Concatenate features from all interaction layers\n",
    "        node_feats_out = torch.cat(node_feats_list, dim=-1)\n",
    "\n",
    "        # Final descriptor\n",
    "        descriptors = {\n",
    "            \"nodes\": {\n",
    "                \"node_env\": node_feats_out,\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return descriptors\n",
    "\n",
    "\n",
    "    \n",
    "def z_one_hot(z, orbitals, nr_bits):\n",
    "    \"\"\"\n",
    "    Generate one-hot encodings from a list of single-value tensors.\n",
    "\n",
    "    Args:\n",
    "        z (list of torch.Tensor): A list of single-value tensors, e.g., [[2], [3], [4], [2], [2], ...].\n",
    "        orbitals (dict): A dictionary mapping numbers to their corresponding values.\n",
    "        nr_bits (int): The number of bits for one-hot encoding.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the one-hot encodings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract values from the list of single-value tensors\n",
    "    node_map={}\n",
    "    k=0\n",
    "    for key in orbitals.keys():\n",
    "        node_map[key]=k\n",
    "        k+=1\n",
    "\n",
    "    indices = [tensor.item() for tensor in z]\n",
    "\n",
    "    # Create an empty tensor for one-hot encoding\n",
    "    one_hot = torch.zeros(len(indices), nr_bits)\n",
    "\n",
    "    # Fill in the one-hot encoding based on the indices\n",
    "    for i, idx in enumerate(indices):\n",
    "        if idx in orbitals:  # Ensure the index exists in orbitals\n",
    "            one_hot[i, int(node_map[idx])] = 1  # Set the corresponding bit to 1\n",
    "        else:\n",
    "            raise ValueError(f\"Index {idx} not found in orbitals.\")\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "def get_object_from_module(class_name, module=\"mace.modules\"):\n",
    "    try:\n",
    "        return getattr(importlib.import_module(module), class_name)\n",
    "    except AttributeError:\n",
    "        return None  # Or raise an error if you prefer\n",
    "\n",
    "# === The matrix readout function ===\n",
    "model = E3nnGraph2Mat(\n",
    "    unique_basis=basis,\n",
    "    irreps=dict(node_feats_irreps=final_irreps, edge_feats_irreps=final_irreps),\n",
    "    symmetric=True,\n",
    "    blocks_symmetry='ij=ji', # This is the symmetry of the matrix we want to compute\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676dd155",
   "metadata": {},
   "source": [
    "# 2. Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c01f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now load two different configurations.\n",
    "\n",
    "embedding_configs = []\n",
    "for i, path in enumerate(paths):\n",
    "    if i==2:\n",
    "        break\n",
    "\n",
    "    # Load the structure config\n",
    "    file = sisl.get_sile(path / \"aiida.fdf\")\n",
    "    geometry = file.read_geometry()\n",
    "\n",
    "    embeddings_config = BasisConfiguration(\n",
    "        point_types=geometry.atoms.Z,\n",
    "        positions=geometry.xyz,\n",
    "        basis=basis,\n",
    "        cell=geometry.cell,\n",
    "        pbc=geometry.pbc,\n",
    "        metadata={\n",
    "            \"device\": device,\n",
    "            \"atom_types\": torch.from_numpy(geometry.atoms.Z), # Unlike point_types, this is not rescaled.\n",
    "        }\n",
    "    )\n",
    "\n",
    "    embedding_configs.append(embeddings_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6351f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph2mat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
