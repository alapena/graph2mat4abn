debug_mode: false
device: "cuda:1"
results_dir: "./results/h_noc_2_continuation"
trained_model_path: "./results/h_noc_2/train_best_model.tar"
dataset:
  train_split_ratio: 0.9
  # test_split_ratio: 0.50
  stratify: true # set to false if max_samples < 200
  # max_samples: null
  seed: 42
  num_unique_z: 2 # Fasten the basis calculation

  # use_previous_dataset: true # Automatically to true if trained_model_path is not null
  use_only: ["2_ATOMS", "8_ATOMS"] # Write dataset subfolders suffix. E.g. ["64_ATOMS"]
  exclude_carbons: true
  custom_dataset: false
  extra_custom_validation: false

orbitals:
  1: 13
  2: 13
  3: 13
  4: 13
  5: 13
  6: 13
  7: 13
  8: 13

environment_representation:
  num_interactions: 2 # Nº layers
  correlation: 3 # Correlation order of the messages
  max_ell: 3 # Expansion order

  num_channels: 128 # default. Determines the size of the model
  max_L: 4 # default. Symmetries of the message. According to hidden_irreps.
  r_max: 10.0 # Cutoff radius
  num_elements: 2 # Nº of chemical elements
  atomic_energies: [0, 0] # For normalization(?). torhc.tensor() # Must match the shape of atomic_numbers
  avg_num_neighbors: 8 # For norm.(?)
  atomic_numbers: [5,7] # (?)

  num_bessel: 15 # Nº radial feats
  num_polynomial_cutoff: 8 # Smothness of the radial cutoff
  hidden_irreps: "10x0e + 10x1o + 10x2e + 10x3o"
  MLP_irreps: 20x0e # Nº hidden dim. of last layer readout MLP
  gate: silu # non-linearity of the last layer readout

model:
  readout_per_interaction: false
  preprocessing_nodes: E3nnInteraction
  preprocessing_edges: E3nnEdgeMessageBlock
  node_operation: E3nnSimpleNodeBlock
  edge_operation: E3nnSimpleEdgeBlock
  # hidden_neurons: [64, 64, 64, 64]

optimizer:
  lr: 1e-3
  # weight_decay: 1e-4
  initial_lr: 1e-4 # Force an initial lr in the scheduler when loading a model.

scheduler:
  type: ReduceLROnPlateau
  step: train_loss # train_loss or val_loss, or None depending on the scheduler
  args:
  kwargs:
    mode: 'min'
    factor: 0.9 
    patience: 100
    cooldown: 0
    min_lr: 1e-9
    eps: 0


  # CosineAnnealingWarmRestarts:
  #   t_0: 20
  #   t_multiplication: 2
  #   eta_min: 1e-25

  # CyclicLR:
  #   base_lr: 1e-9
  #   max_lr: 1e-3
  #   step_size_up: 100
  #   cycle_momentum: False
  #   mode: triangular

  # OneCycleLR:
  #   max_lr: 1e-3                     # Upper learning rate boundary
  #   epochs_cycle: 50
  #   pct_start: 0.3                # Fraction of steps spent increasing LR (Default 0.3)
  #   anneal_strategy: 'cos'      # Annealing strategy
  #   div_factor: 1e2               # max_lr / initial_lr
  #   final_div_factor: 1e2        # initial_lr / min_lr

  # ReduceLROnPlateau:
  #   mode: 'min'
  #   factor: 0.1 
  #   patience: 20
  #   cooldown: 0
  #   min_lr: 0
  #   eps: 0
  

trainer:
  batch_size: 70
  checkpoint_freq: 1000

  live_plot: true
  live_plot_freq: 1

  live_plot_matrix: false
  live_plot_matrix_freq: 1000
  live_plot_matrices_num: 1

  num_epochs: 500000
  loss_function: block_type_mse # elementwise_mse, block_type_mse, block_type_mse_threshold

  keep_in_memory: true
  rotating_pool: false
  rotating_pool_size: 400

  matrix: hamiltonian # "hamiltonian", "tim", "overlap"
  
