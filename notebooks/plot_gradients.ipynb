{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a4e4a8",
   "metadata": {},
   "source": [
    "This script only works with the last models I trained (e.g. h_crystalls_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464ce8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ICN2/alapena/miniconda3/envs/graph2mat_upt/lib/python3.12/site-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuequivariance or cuequivariance_torch is not available. Cuequivariance acceleration will be disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/alapena/GitHub/graph2mat4abn')\n",
    "import os\n",
    "os.chdir('/home/ICN2/alapena/GitHub/graph2mat4abn') # Change to the root directory of the project\n",
    "\n",
    "from graph2mat4abn.tools.import_utils import load_config, get_object_from_module\n",
    "from graph2mat4abn.tools.tools import get_basis_from_structures_paths, get_kwargs, load_model\n",
    "from graph2mat4abn.tools.scripts_utils import get_model_dataset, init_mace_g2m_model\n",
    "from graph2mat4abn.tools.script_plots import update_loss_plots, plot_grad_norms\n",
    "from pathlib import Path\n",
    "from e3nn import o3\n",
    "from mace.modules import MACE, RealAgnosticResidualInteractionBlock\n",
    "from graph2mat.models import MatrixMACE\n",
    "from graph2mat.bindings.e3nn import E3nnGraph2Mat\n",
    "import torch\n",
    "import warnings\n",
    "from graph2mat import BasisTableWithEdges\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"The TorchScript type system doesn't support\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*is not a known matrix type key.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bace8549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Basis computation.\n",
      "Number of structures to look on: 629\n",
      "Looking for unique atoms in each structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 58.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found enough basis points. Breaking the search...\n",
      "Found enough basis points. Breaking the search...\n",
      "Found the following atomic numbers: [7, 5]\n",
      "Corresponding path indices: [0, 0]\n",
      "Basis with 2 elements built!\n",
      "\n",
      "Basis for atom 0.\n",
      "\tAtom type: 5\n",
      "\tBasis: ((2, 0, 1), (2, 1, -1), (1, 2, 1))\n",
      "\tBasis convention: siesta_spherical\n",
      "\tR: [3.02420918 2.02341372 3.73961942 3.73961942 3.73961942 2.51253945\n",
      " 2.51253945 2.51253945 3.73961942 3.73961942 3.73961942 3.73961942\n",
      " 3.73961942]\n",
      "\n",
      "Basis for atom 1.\n",
      "\tAtom type: 7\n",
      "\tBasis: ((2, 0, 1), (2, 1, -1), (1, 2, 1))\n",
      "\tBasis convention: siesta_spherical\n",
      "\tR: [2.25704422 1.4271749  2.78012609 2.78012609 2.78012609 1.75309697\n",
      " 1.75309697 1.75309697 2.78012609 2.78012609 2.78012609 2.78012609\n",
      " 2.78012609]\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/ICN2/alapena/miniconda3/envs/graph2mat_upt/lib/python3.12/site-packages/mace/modules/blocks.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(atomic_energies, dtype=torch.get_default_dtype()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "LR Scheduler:  ReduceLROnPlateau\n",
      "Arguments:  None\n",
      "Keyword arguments:  {'cooldown': 0, 'eps': 0.0, 'factor': 0.1, 'min_lr': 0.0, 'mode': 'min', 'patience': 20}\n",
      "Using Loss function <class 'graph2mat.core.data.metrics.block_type_mse'>\n"
     ]
    }
   ],
   "source": [
    "# The current model:\n",
    "model_dir = Path(\"results/h_noc_2\")\n",
    "filename = \"train_best_model.tar\"\n",
    "config = load_config(model_dir / \"config.yaml\")\n",
    "\n",
    "# Basis generation (needed to initialize the model)\n",
    "train_paths, val_paths = get_model_dataset(model_dir, verbose=False)\n",
    "paths = train_paths + val_paths\n",
    "basis = get_basis_from_structures_paths(paths, verbose=True, num_unique_z=config[\"dataset\"].get(\"num_unique_z\", None))\n",
    "table = BasisTableWithEdges(basis)\n",
    "\n",
    "print(\"Initializing model...\")\n",
    "model, optimizer, lr_scheduler, loss_fn = init_mace_g2m_model(config, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0091df55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in epoch 24782 with training loss 11333.201171875 and validation loss 35070.49609375.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_path = model_dir / filename\n",
    "model, checkpoint, optimizer, lr_scheduler = load_model(model, optimizer, model_path, lr_scheduler=None, initial_lr=None, device='cpu')\n",
    "history = checkpoint[\"history\"]\n",
    "print(f\"Loaded model in epoch {checkpoint[\"epoch\"]} with training loss {checkpoint[\"train_loss\"]} and validation loss {checkpoint[\"val_loss\"]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7227bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1923520622733568"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(history[\"epoch_times\"]) / 60 / 60 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdeb3bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.616449494560563"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".1923520622733568*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbad9f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.98696967363378"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".616449494560563*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75f2d8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.2181804180268"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".98696967363378*60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59777db6",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da00a1",
   "metadata": {},
   "source": [
    "## Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d05ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_loss_plots(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c482cf8b",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grad_norms(history['grad_norms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820abf6",
   "metadata": {},
   "source": [
    "Good! It does not seem that there is any vanishing gradients problem. All the gradients remainet ~ctt during the training, and the model is still learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f70916d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph2mat_upt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
